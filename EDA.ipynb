{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"\n## General information\n\nCorrectly predicting earthquakes is very important for preventing deaths and damage to infrastructure. In this competition we try to predict time left to the next laboratory earthquake based on seismic signal data.\n\nTraining data represents one huge signal, but in test data we have many separate chunks, for each of which we need to predict time to failure.\n\nThis kernel is dedicated to exploration of LANL Earthquake Prediction Challenge.\n\n\n![](https://images.spot.im/v1/production/vbwfwtqv6krdqcmxu2k9)","metadata":{"_uuid":"041eccd08aafc3e46f18b1b7a1b80a2b28a0003f"}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom tqdm import tqdm_notebook\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import NuSVR, SVR\nfrom sklearn.metrics import mean_absolute_error\npd.options.display.precision = 15\n\nimport lightgbm as lgb\nimport xgboost as xgb\nimport time\nimport datetime\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import StratifiedKFold, KFold, RepeatedKFold\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\nfrom sklearn.linear_model import Ridge, RidgeCV\nfrom scipy import stats\nimport math\nimport gc\nfrom catboost import CatBoostRegressor\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom itertools import chain","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#new additions\nfrom sklearn.linear_model import LinearRegression\nfrom scipy.signal import hilbert\nfrom scipy.signal import hann\nfrom scipy.signal import convolve\nfrom sklearn.kernel_ridge import KernelRidge\nfrom functools import partial","metadata":{"_uuid":"b669a45482d15b09f26da7b89aec6246582bebaf","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"root_folder = \"C:\\\\paritosh\\\\kaggle\\\\LANL-Earthquake-Prediction\\\\\"\nroot_folder = r\"../input/\"\nz_score_cutoff = 4\nrows = 150_000 #no of rows to aggregate","metadata":{"_uuid":"54a2e2039f071f649320a88a4135498c69b0533b","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ntrain = pd.read_csv(root_folder + 'train.csv', dtype={'acoustic_data': np.int16, 'time_to_failure': np.float32})","metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"_uuid":"d1d33084c48168284d75d0da861009789cb081d4","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.shape","metadata":{"_uuid":"7ad2361064a108547c6c9e7277dd920583cbe165","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We have 629 million rows! Huge data, so let's plot a sample of it.","metadata":{"_uuid":"a2c436d0c0abbefac651799f63ad52554a5bcbd4"}},{"cell_type":"code","source":"train_acoustic_data_small = train['acoustic_data'].values[::50]\ntrain_time_to_failure_small = train['time_to_failure'].values[::50]\n\nfig, ax1 = plt.subplots(figsize=(16, 8))\nplt.title(\"Trends of acoustic_data and time_to_failure. 2% of data (sampled)\")\nplt.plot(train_acoustic_data_small, color='b')\nax1.set_ylabel('acoustic_data', color='b')\nplt.legend(['acoustic_data'])\nax2 = ax1.twinx()\nplt.plot(train_time_to_failure_small, color='g')\nax2.set_ylabel('time_to_failure', color='g')\nplt.legend(['time_to_failure'], loc=(0.875, 0.9))\nplt.grid(False)\n\ndel train_acoustic_data_small\ndel train_time_to_failure_small","metadata":{"_kg_hide-input":true,"_uuid":"95b8d85ba4348b7ff4ce389d3a44cebf6e549e6a","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax1 = plt.subplots(figsize=(16, 8))\nplt.title(\"Trends of acoustic_data and time_to_failure. First 2% of data\")\nplt.plot(train['acoustic_data'].values[:12582910], color='b')\nax1.set_ylabel('acoustic_data', color='b')\nplt.legend(['acoustic_data'])\nax2 = ax1.twinx()\nplt.plot(train['time_to_failure'].values[:12582910], color='g')\nax2.set_ylabel('time_to_failure', color='g')\nplt.legend(['time_to_failure'], loc=(0.875, 0.9))\nplt.grid(False)","metadata":{"_kg_hide-input":true,"_uuid":"4149effc6e32ee4a5c4bab817af080c3696a0288","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"On the first plot you can see 2% of all data (all data skipping each 50 rows). We can see that usually acoustic data shows huge fluctuations just before the failure and the nature of data is cyclical.\n\nOn the second plot we see first 2% of the data. It seems that at first the signal has huge fluctuations for a short time, then it lowers and after some time the earthquake occurs. I think it will be quite diffucult to distinguish target values properly","metadata":{"_uuid":"584a3f891dbfee0b82ab8c4dfdab5fb35c88d07b"}},{"cell_type":"markdown","source":"### Feature engineering\n\nLet's create some new features.\n\nWhy 150000? Test segments are 150000 each. At first I create features similar to baseline kernel, but with more aggregations.","metadata":{"_uuid":"c4972c4a8bc047aa05d909f0b447340f3c5353b3"}},{"cell_type":"code","source":"def add_areas(df_segment, dx=2.5e-07):\n    '''calculates auc upto every point in a segment of acoustic data\n    returns an array containing auc upto that point in array sequence\n    '''\n    x = df_segment['acoustic_data'].values\n    x_shifted = np.zeros_like(x)\n    x_shifted[1:] = x[:-1]\n    x_stacked = np.stack((x_shifted, x), axis=-1)\n    area_pairs = np.trapz(x_stacked, dx=dx)\n    area_pairs.shape\n    result = np.cumsum(area_pairs)\n    return result\n\n\ndef tail_sizer(big_tail_factor=3, small_tail_factor=15, rows=rows):\n    '''determines what factor or portion of total items forms the tails\n    '''\n    big_tail = rows // big_tail_factor\n    small_tail = rows // small_tail_factor\n    tails = [big_tail, small_tail]\n    return tails","metadata":{"_uuid":"1ed9186274593e3ee3e9982ca51ccd1edc01117b","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def classic_sta_lta_mean(x, length_sta, length_lta):    \n    sta = np.cumsum(x ** 2)    \n    # Convert to float\n    sta = np.require(sta, dtype=np.float)    \n    # Copy for LTA\n    lta = sta.copy()    \n    # Compute the STA and the LTA\n    sta[length_sta:] = sta[length_sta:] - sta[:-length_sta]\n    sta /= length_sta\n    lta[length_lta:] = lta[length_lta:] - lta[:-length_lta]\n    lta /= length_lta    \n    # Pad zeros\n    sta[:length_lta - 1] = 0    \n    # Avoid division by zero by setting zero values to tiny float\n    dtiny = np.finfo(0.0).tiny\n    idx = lta < dtiny\n    lta[idx] = dtiny\n    return np.mean(sta / lta)\n\ndef sta_lta_func_maker(calculate_vals = False):\n    sta_lta_len_pairs = [\n                        (500, 10000),\n                        (5000, 100000),\n                        (3333, 6666),\n                        (10000, 25000),\n                        (50, 1000),\n                        (100, 5000),\n                        (333, 666),\n                        (4000, 10000),\n                        ]\n    result = {}\n    for i, sta_lta in enumerate(sta_lta_len_pairs):\n        result[f'classic_sta_lta{i+1}_mean'] = partial(classic_sta_lta_mean, \n                                                      length_sta = sta_lta[0],\n                                                      length_lta = sta_lta[1],\n                                                      )\n    if not calculate_vals:\n        return list(result.keys())\n    return result","metadata":{"_uuid":"973ba8f36bae35d24f2a2f6442ba16cbd6324865","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def MA_func_maker(x = 0, calculate_vals = True):\n    no_of_std = 3\n    x = pd.Series(x)\n    result = {}\n    result['MA_700MA_std_mean'] = x.rolling(window=700).std().mean()\n    Moving_average_700_mean = x.rolling(window=700).mean().mean(skipna=True)\n    result['MA_700MA_BB_high_mean'] = (Moving_average_700_mean\n                                      + no_of_std * result['MA_700MA_std_mean'])\n    result['MA_700MA_BB_low_mean'] = (Moving_average_700_mean\n                                      - no_of_std * result['MA_700MA_std_mean'])\n    result['MA_400MA_std_mean'] = x.rolling(window=400).std().mean()\n    #wtf arent these bottom ones wrong\n    result['MA_400MA_BB_high_mean'] = (Moving_average_700_mean\n                                      + no_of_std * result['MA_400MA_std_mean'])\n    result['MA_400MA_BB_low_mean'] = (Moving_average_700_mean\n                                      - no_of_std * result['MA_400MA_std_mean'])\n    result['MA_1000MA_std_mean'] = x.rolling(window=1000).std().mean()\n    if not calculate_vals:\n        return list(result.keys())\n    return result","metadata":{"_uuid":"af84559473a1d2c9e0b36bfb5d579b5c323f6045","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def rolling_func_maker(x = 0, calculate_vals = True):\n    windows = [10, 100, 1000]\n    x = pd.Series(x)\n    result = {}\n    for window in windows:\n        x_roll_std = x.rolling(window).std().dropna().values\n        x_roll_mean = x.rolling(window).mean().dropna().values\n        temp = {\n                'std': x_roll_std,\n                'mean': x_roll_mean,\n                }\n        if not calculate_vals:\n            temp = {k: np.array([0]) for k in temp}\n        for k, v in temp.items():\n            result[f'ave_roll_{k}_{window}'] = v.mean()\n            result[f'std_roll_{k}_{window}'] = v.std()\n            result[f'max_roll_{k}_{window}'] = v.max()\n            result[f'min_roll_{k}_{window}'] = v.min()\n            result[f'q01_roll_{k}_{window}'] = np.quantile(v, 0.01)\n            result[f'q05_roll_{k}_{window}'] = np.quantile(v, 0.05)\n            result[f'q95_roll_{k}_{window}'] = np.quantile(v, 0.95)\n            result[f'q99_roll_{k}_{window}'] = np.quantile(v, 0.99)\n            result[f'av_change_abs_roll_{k}_{window}'] = np.mean(np.diff(v))\n            result[f'av_change_abs_roll_{k}_{window}'] = np.mean(np.nonzero(\n                                                                            (np.diff(v) / v[:-1])\n                                                                            )[0])\n    if not calculate_vals:\n        return list(result.keys())\n    return result","metadata":{"_uuid":"1834a172ebc4108d9580309c97b61bbf8412d664","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def column_namer(prefix=''):\n    '''\n    basic columns: passed as is\n    columns: modified to make special cols\n    based on heads and tails in aggregation\n    prefix: what to add to column names for every new feature\n    returns: list of basic columns, columns, and special columns.\n    '''\n    prefix = prefix.lower()\n    if prefix == '':\n        basic_columns = [\n                'av_change_abs',\n                'av_change_rate',\n                'abs_max',\n#                'abs_min',\n                #new additions\n                'abs_mean',\n                'abs_std',\n                'max_to_min',\n                'max_to_min_diff',\n                'count_big',\n                'sum',\n                'q99',\n                'q95',\n                'q5',\n                'q1',\n                'abs_q99',\n                'abs_q95',\n                'abs_q5',\n                'abs_q1',\n                'trend',\n                'abs_trend',\n                'mad',\n                'kurt',\n                'skew',\n                'median',\n                'Hilbert_mean',\n                'Hann_window_mean',\n                'exp_Moving_average_300_mean',\n                'exp_Moving_average_3000_mean',\n                'exp_Moving_average_30000_mean',\n                ]\n        columns = [\n                    'avg',\n                    'std',\n                    'max',\n                    'min',\n                    'zscore_counts',\n                    'zscore_sum',\n                    ]\n\n    if prefix == \"on_auc\":\n        basic_columns = [\n                'av_change_abs',\n                'av_change_rate',\n                'abs_max',\n                'abs_min',\n                ]\n        columns = [\n                    'avg',\n                    'std',\n                    'max',\n                    'min',\n#                    'zscore_counts',\n#                    'zscore_sum',\n                    ]\n    sta_lta_cols = sta_lta_func_maker()\n    \n    basic_columns += sta_lta_cols #can be changed as needed\n    if prefix:\n        basic_columns = [f\"{prefix}_{col}\" for col in basic_columns]\n        columns = [f\"{prefix}_{col}\" for col in columns]\n\n    tails = tail_sizer()\n    special_cols = []\n    for col in columns:\n        for tail in tails:\n            special_cols.append(f'{col}_first_{tail}')\n            special_cols.append(f'{col}_last_{tail}')\n    \n    MA_cols = MA_func_maker(calculate_vals = False)\n    rolling_cols = rolling_func_maker(calculate_vals = False)\n    return basic_columns, columns, special_cols, MA_cols, rolling_cols","metadata":{"_uuid":"47e34ba80b0e3a978681300ac24c55896f393cbd","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def compute(x, prefix, rows=rows, z_score_cutoff=4):\n    '''Takes a numpy array of a dataframe segment to\n    aggregate into a single row as dict\n    '''\n    basic_columns, columns, special_cols, MA_cols, rolling_cols = column_namer(prefix)\n    #calculations needed\n    assert len(x) == rows\n\n    def f_zscore_counts(x, z_score_cutoff=z_score_cutoff):\n        zscore_seg = np.abs(stats.zscore(x))\n        return np.sum(zscore_seg > z_score_cutoff)\n\n    def f_zscore_sum(x, z_score_cutoff=z_score_cutoff):\n        zscore_seg = np.abs(stats.zscore(x))\n        return np.sum(zscore_seg[zscore_seg > z_score_cutoff])\n    \n    def calc_change_rate(x):\n        change = (np.diff(x) / x[:-1])\n        change = change[np.nonzero(change)[0]]\n        change = change[~np.isnan(change)]\n        change = change[change != -np.inf]\n        change = change[change != np.inf]\n        return np.mean(change)\n    \n    def add_trend_feature(arr, abs_values=False):\n        idx = np.array(range(len(arr)))\n        if abs_values:\n            arr = np.abs(arr)\n        lr = LinearRegression()\n        lr.fit(idx.reshape(-1, 1), arr)\n        return lr.coef_[0]\n    \n    ewma = pd.Series.ewm\n    \n    mapping = {\n        'avg': lambda x: x.mean(),\n        'std': lambda x: x.std(),\n        'max': lambda x: x.max(),\n        'min': lambda x: x.min(),\n        'av_change_abs': lambda x: np.mean(np.diff(x)),\n        'av_change_rate': calc_change_rate, #modified\n        'abs_max': lambda x: np.abs(x).max(),\n        'abs_min': lambda x: np.abs(x).min(),\n        'abs_mean': lambda x: np.abs(x).mean(), #new\n        'abs_std': lambda x: np.abs(x).std(), #new\n        'zscore_counts': f_zscore_counts,\n        'zscore_sum': f_zscore_sum,\n        #new additions\n        'max_to_min': lambda x: x.max() / np.abs(x.min()),\n        'max_to_min_diff': lambda x: x.max() - np.abs(x.min()),\n        'count_big': lambda x: len(x[np.abs(x) > 500]),\n        'sum': lambda x: x.sum(),\n        'q99': lambda x: np.quantile(x, 0.99),\n        'q95': lambda x: np.quantile(x, 0.95),\n        'q5': lambda x: np.quantile(x, 0.05),\n        'q1': lambda x: np.quantile(x, 0.01),\n        'abs_q99': lambda x: np.quantile(np.abs(x), 0.99),\n        'abs_q95': lambda x: np.quantile(np.abs(x), 0.95),\n        'abs_q5': lambda x: np.quantile(np.abs(x), 0.05),\n        'abs_q1': lambda x: np.quantile(np.abs(x), 0.01),\n        'trend': lambda x: add_trend_feature(x),\n        'abs_trend': lambda x: add_trend_feature(x, abs_values=True),\n        'mad': lambda x: pd.Series(x).mad(),\n        'kurt': lambda x: pd.Series(x).kurtosis(),\n        'skew': lambda x: pd.Series(x).skew(),\n        'median': lambda x: np.median(x),\n        'Hilbert_mean': lambda x: np.abs(hilbert(x)).mean(),\n        'Hann_window_mean': lambda x: (convolve(x, hann(150), mode='same') / sum(hann(150))).mean(),\n        'Moving_average_700_mean': lambda x: x.rolling(window=700).mean().mean(skipna=True),\n        'exp_Moving_average_300_mean': lambda x: (ewma(pd.Series(x), span=300).mean()).mean(skipna = True),\n        'exp_Moving_average_3000_mean': lambda x: (ewma(pd.Series(x), span=3000).mean()).mean(skipna = True),\n        'exp_Moving_average_30000_mean': lambda x: (ewma(pd.Series(x), span=30000).mean()).mean(skipna = True),\n        'iqr': lambda x: np.subtract(*np.percentile(x, [75, 25])),\n        'q999': lambda x: np.quantile(x,0.999),\n        'q001': lambda x: np.quantile(x,0.001),\n        'ave10': lambda x: stats.trim_mean(x, 0.1),\n        \n        }\n    sta_lta_mappings = sta_lta_func_maker(calculate_vals=True)\n    mapping.update(sta_lta_mappings)\n    \n    if prefix:\n        mapping = {f\"{prefix}_{k}\": v for k, v in mapping.items()}\n\n    result = {k: mapping[k](x) for k in basic_columns + columns}\n    \n    if MA_cols:\n        MA_results = MA_func_maker(x)\n        result.update(MA_results)\n    \n    if rolling_cols:\n        rolling_results = rolling_func_maker(x)\n        result.update(rolling_results)\n        \n    tails = tail_sizer()\n\n    for col in columns:\n        for tail in tails:\n            result[f'{col}_first_{tail}'] = mapping[col](x[: tail])\n            result[f'{col}_last_{tail}'] = mapping[col](x[-tail:])\n    return result\n\n\n","metadata":{"_uuid":"5f686e4f5aa92260ad142c6dbd9654f9bc57f48e","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def feature_aggregation(calculate_vals=False, df_segment=None):\n    '''Takes a dataframe segment, and returns column names if vals is false,\n    otherwise returns the dict of results on which to update df\n    '''\n    #prefixes = ['', 'on_auc']\n    prefixes = ['']\n    all_cols = [list(chain.from_iterable(column_namer(prefix)))\n                for prefix in prefixes]\n    all_cols = list(chain.from_iterable(all_cols))\n    if not calculate_vals:\n        return all_cols\n\n    result_dict = {}\n    for prefix in prefixes:\n        if prefix == '':\n            x = df_segment['acoustic_data'].values\n        elif prefix == 'on_auc':\n            x = add_areas(df_segment)\n        result_dict.update(compute(x, prefix))\n    return result_dict","metadata":{"_uuid":"890c25ae31af879fc6d8a945cde1b697302db528","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"segments = int(np.floor(train.shape[0] / rows))\nsubsampling_size = 2\noffset = int(np.floor((train.shape[0] - (rows// subsampling_size)) / rows))\nsegments += offset\nprint(segments)","metadata":{"_uuid":"e8d5bdf88a8a52d13fd390c34ad42dbf10f8901d","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a training file with simple derived features\nX_tr = pd.DataFrame(index=range(segments), dtype=np.float64,\n                    columns=feature_aggregation()\n                   )\ny_tr = pd.DataFrame(index=range(segments), dtype=np.float64,\n                       columns=['time_to_failure'])\n\ntotal_mean = train['acoustic_data'].mean()\ntotal_std = train['acoustic_data'].std()\ntotal_max = train['acoustic_data'].max()\ntotal_min = train['acoustic_data'].min()\ntotal_sum = train['acoustic_data'].sum()\ntotal_abs_max = np.abs(train['acoustic_data']).sum()\n\nfor segment in tqdm_notebook(range(segments)):\n    seg = train.iloc[segment*(rows//subsampling_size):segment*(rows//subsampling_size)+rows]\n    y = seg['time_to_failure'].values[-1]\n    y_tr.loc[segment, 'time_to_failure'] = y\n    \n    result = feature_aggregation(calculate_vals = True, df_segment = seg)\n    for k, v in result.items():\n        X_tr.loc[segment, k] = v\n","metadata":{"_uuid":"fd04dc4ad94e04b34b125277b05e650fd7531d85","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_tr.to_csv(\"X_tr.csv\")\ny_tr.to_csv(\"Y_tr.csv\")","metadata":{"_uuid":"57d2be10901474d1f3cfa8f2da8aa601a220fc36","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f'{X_tr.shape[0]} samples in new train data.')","metadata":{"_uuid":"c03e361f5390471f0281367b9520933c7c999223","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's see all new features","metadata":{"_uuid":"d4539c37e6282084d760678b1d70a2b76e7fe3d1"}},{"cell_type":"code","source":"plt.figure(figsize=(len(X_tr.columns) + 2, 24))\nfor i, col in enumerate(X_tr.columns):\n    plt.subplot(math.ceil(len(X_tr.columns)/4), 4, i + 1)\n    plt.plot(X_tr[col], color='blue')\n    plt.title(col)\n    ax1.set_ylabel(col, color='b')\n    # plt.legend([col])\n    ax2 = ax1.twinx()\n    plt.plot(y_tr, color='g')\n    ax2.set_ylabel('time_to_failure', color='g')\n    plt.legend([col, 'time_to_failure'], loc=(0.875, 0.9))\n    plt.grid(False)","metadata":{"_kg_hide-input":true,"_uuid":"b0f0a3d919074b0630a655dc560e7fc2287e8017","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_tr.dropna(how='any', axis = 0, inplace = True)","metadata":{"_uuid":"93f35564c186fd3b8fa16df0df9d33ae927995ef","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(X_tr)","metadata":{"_uuid":"ed0531ef1754dd7b8a9ef0fcc26b7c54bc08cd3b","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_tr.replace([np.inf, -np.inf], np.nan, inplace = True)","metadata":{"_uuid":"f6d0fac2ae7a6098c64fdf57edf7d78f57f7b26a","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_tr.dropna(how='any', axis = 0, inplace = True)","metadata":{"_uuid":"cce6f2d2364186b7ab85c2c6db943f7864e57591","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(X_tr)","metadata":{"_uuid":"b4e2c57dcc0f7bacacf83a66a6bb63a39c73858a","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scaler = StandardScaler()\nscaler.fit(X_tr)\nX_train_scaled = pd.DataFrame(scaler.transform(X_tr), columns=X_tr.columns)","metadata":{"_uuid":"33d7dc2509af6eb317111b9b6b0021c0af3c75e0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = pd.read_csv(root_folder + 'sample_submission.csv', index_col='seg_id')\nX_test = pd.DataFrame(columns=X_tr.columns, dtype=np.float64, index=submission.index)\nplt.figure(figsize=(22, 16))\n\nfor i, seg_id in enumerate(tqdm_notebook(X_test.index)):\n    seg = pd.read_csv(root_folder + 'test/' + seg_id + '.csv')\n    \n    result = feature_aggregation(calculate_vals = True, df_segment = seg)\n    for k, v in result.items():\n        X_test.loc[seg_id, k] = v\n    \n    if i < 24:\n        plt.subplot(7, 4, i + 1)\n        plt.plot(seg['acoustic_data'])\n        plt.title(seg_id)\n    \nX_test_scaled = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns)","metadata":{"_uuid":"8a8fefba8af693620aa3598ac3de8f2ad521bbf4","scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_test.describe()","metadata":{"_uuid":"a9f9a2eb5de478fff0f2113604f95ce60d9becea","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n_fold = 5\nfolds = KFold(n_splits=n_fold, shuffle=True, random_state=11)","metadata":{"_uuid":"9172a3903b178e151d8aefbc68e01426a39e9e2e","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_model(X=X_train_scaled, X_test=X_test_scaled, y=y_tr, params=None, folds=folds, model_type='lgb', plot_feature_importance=False, model=None):\n\n    oof = np.zeros(len(X))\n    prediction = np.zeros(len(X_test))\n    scores = []\n    feature_importance = pd.DataFrame()\n    for fold_n, (train_index, valid_index) in enumerate(folds.split(X)):\n        print('Fold', fold_n, 'started at', time.ctime())\n        X_train, X_valid = X.iloc[train_index], X.iloc[valid_index]\n        y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n        \n        if model_type == 'lgb':\n            model = lgb.LGBMRegressor(**params, n_estimators = 20000, nthread = 4, n_jobs = -1)\n            model.fit(X_train, y_train, \n                    eval_set=[(X_train, y_train), (X_valid, y_valid)], eval_metric='mae',\n                    verbose=10000, early_stopping_rounds=200)\n            \n            y_pred_valid = model.predict(X_valid)\n            y_pred = model.predict(X_test, num_iteration=model.best_iteration_)\n            \n        if model_type == 'xgb':\n            train_data = xgb.DMatrix(data=X_train, label=y_train, feature_names=X_tr.columns)\n            valid_data = xgb.DMatrix(data=X_valid, label=y_valid, feature_names=X_tr.columns)\n\n            watchlist = [(train_data, 'train'), (valid_data, 'valid_data')]\n            model = xgb.train(dtrain=train_data, num_boost_round=20000, evals=watchlist, early_stopping_rounds=200, verbose_eval=500, params=params)\n            y_pred_valid = model.predict(xgb.DMatrix(X_valid, feature_names=X_tr.columns), ntree_limit=model.best_ntree_limit)\n            y_pred = model.predict(xgb.DMatrix(X_test, feature_names=X_tr.columns), ntree_limit=model.best_ntree_limit)\n            \n        if model_type == 'rcv':\n            model = RidgeCV(alphas=(0.01, 0.1, 1.0, 10.0, 100.0), scoring='neg_mean_absolute_error', cv=3)\n            model.fit(X_train, y_train)\n            print(model.alpha_)\n\n            y_pred_valid = model.predict(X_valid).reshape(-1,)\n            score = mean_absolute_error(y_valid, y_pred_valid)\n            print(f'Fold {fold_n}. MAE: {score:.4f}.')\n            print('')\n            \n            y_pred = model.predict(X_test).reshape(-1,)\n        \n        if model_type == 'sklearn':\n            model = model\n            model.fit(X_train, y_train)\n            \n            y_pred_valid = model.predict(X_valid).reshape(-1,)\n            score = mean_absolute_error(y_valid, y_pred_valid)\n            print(f'Fold {fold_n}. MAE: {score:.4f}.')\n            print('')\n            \n            y_pred = model.predict(X_test).reshape(-1,)\n        \n        if model_type == 'cat':\n            model = CatBoostRegressor(iterations=20000,  eval_metric='MAE', **params)\n            model.fit(X_train, y_train, eval_set=(X_valid, y_valid), cat_features=[], use_best_model=True, verbose=False)\n\n            y_pred_valid = model.predict(X_valid)\n            y_pred = model.predict(X_test)\n        \n        oof[valid_index] = y_pred_valid.reshape(-1,)\n        scores.append(mean_absolute_error(y_valid, y_pred_valid))\n\n        prediction += y_pred    \n        \n        if model_type == 'lgb':\n            # feature importance\n            fold_importance = pd.DataFrame()\n            fold_importance[\"feature\"] = X.columns\n            fold_importance[\"importance\"] = model.feature_importances_\n            fold_importance[\"fold\"] = fold_n + 1\n            feature_importance = pd.concat([feature_importance, fold_importance], axis=0)\n\n    prediction /= n_fold\n    \n    print('CV mean score: {0:.4f}, std: {1:.4f}.'.format(np.mean(scores), np.std(scores)))\n    \n    if model_type == 'lgb':\n        feature_importance[\"importance\"] /= n_fold\n        if plot_feature_importance:\n            cols = feature_importance[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(\n                by=\"importance\", ascending=False)[:50].index\n\n            best_features = feature_importance.loc[feature_importance.feature.isin(cols)]\n\n            plt.figure(figsize=(16, 12));\n            sns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\", ascending=False));\n            plt.title('LGB Features (avg over folds)');\n        \n            return oof, prediction, feature_importance\n        return oof, prediction\n    \n    else:\n        return oof, prediction","metadata":{"_kg_hide-input":true,"_uuid":"815e13e6c0a78fc3fa461d4c823c97b185838a7e","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"params = {'num_leaves': 54,\n         'min_data_in_leaf': 79,\n         'objective': 'huber',\n         'max_depth': -1,\n         'learning_rate': 0.01,\n         \"boosting\": \"gbdt\",\n         # \"feature_fraction\": 0.8354507676881442,\n         \"bagging_freq\": 3,\n         \"bagging_fraction\": 0.8126672064208567,\n         \"bagging_seed\": 11,\n         \"metric\": 'mae',\n         \"verbosity\": -1,\n         'reg_alpha': 1.1302650970728192,\n         'reg_lambda': 0.3603427518866501\n         }\noof_lgb, prediction_lgb, feature_importance = train_model(params=params, model_type='lgb', plot_feature_importance=True)","metadata":{"_uuid":"c532c79cb6f3417c3a9eec579979aaea91a0aa10","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xgb_params = {'eta': 0.04, 'max_depth': 10, 'subsample': 0.9, #'colsample_bytree': 0.8, \n          'objective': 'reg:linear', 'eval_metric': 'mae', 'silent': True, 'nthread': 4}\noof_xgb, prediction_xgb = train_model(params=xgb_params, model_type='xgb')","metadata":{"_uuid":"2d01a97d68c4031b3b2d11974a17b2e7053efc88","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_test.info()","metadata":{"_uuid":"acb7a9c221e4b2d9bf7463c0c34cbae829654c08","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = NuSVR(gamma='scale', nu=0.7, C=10.0)\noof_svr, prediction_svr = train_model(params=None, model_type='sklearn', model=model)","metadata":{"_uuid":"076994056cdd612e6628f980a31415547649019a","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Interestingly, if we compare the results of training on this data with results of training on smaller dataset (as in official baseline), then we can see that MAE for XGB model became less than MAE of LGB model.\n\nNow let's see how do our models perform","metadata":{"_uuid":"4e3847381aebc9d77e1cfa12b5d16d51e09d46a0"}},{"cell_type":"code","source":"plt.figure(figsize=(16, 8))\nplt.plot(y_tr, color='g', label='y_train')\nplt.plot(oof_lgb, color='b', label='lgb')\nplt.plot(oof_xgb, color='teal', label='xgb')\nplt.plot(oof_svr, color='red', label='svr')\nplt.plot((oof_lgb + oof_xgb + oof_svr) / 3, color='gold', label='blend')\nplt.legend();\nplt.title('Predictions vs actual');","metadata":{"_uuid":"93248680af51c40093dcdc24cc2f8f1d4d70de44","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that models can't predict high values well, but additional data is predicted much better.","metadata":{"_uuid":"b917ae3354e6b1576f1cea218a94b1533bfde5dd"}},{"cell_type":"code","source":"prediction_lgb[:10], prediction_xgb[:10], prediction_svr[:10]","metadata":{"_uuid":"aed2c67cf5b7596b785bb85b0943ee9fef6e8187","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission['time_to_failure'] = (prediction_lgb + prediction_xgb + prediction_svr) / 3\nprint(submission.head())\nsubmission.to_csv('submission.csv')","metadata":{"_uuid":"acf417f3540ca69267b1088e199e4ae358b36439","scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"_uuid":"f1dbf857b32731eb4724e1ac528be1260772d497","trusted":true},"execution_count":null,"outputs":[]}]}